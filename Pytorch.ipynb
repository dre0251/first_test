{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-06T01:52:56.991438Z",
     "start_time": "2025-03-06T01:52:56.977149Z"
    }
   },
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T01:54:35.458316Z",
     "start_time": "2025-03-06T01:54:35.428480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#数据预处理\n",
    "LEARNING_RATE = 0.1\n",
    "n_epochs = 100\n",
    "n_points = 10\n",
    "data = torch.rand(n_points, 2) * 2 - 1 \n",
    "labels = (data.norm(dim = 1) > 0.7).float().unsqueeze(1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data.to(device)\n",
    "labels.to(device)\n",
    "\n",
    "#创建模型类\n",
    "class CircleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2,20)\n",
    "        self.layer2 = nn.Linear(20,1)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "        \n",
    "#实例化\n",
    "model = CircleClassifier()\n",
    "model = model.to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad() #梯度归零\n",
    "    predictions = model(data)\n",
    "    loss = loss_fn(predictions, labels)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item(): .4f}\")\n"
   ],
   "id": "197e1f0e10941a64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:  0.7203\n",
      "Epoch 20, Loss:  0.7203\n",
      "Epoch 40, Loss:  0.7203\n",
      "Epoch 60, Loss:  0.7203\n",
      "Epoch 80, Loss:  0.7203\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:32:22.673675Z",
     "start_time": "2025-03-06T05:32:22.665129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#超参数\n",
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "block_size = 4"
   ],
   "id": "10e0c279e7f95895",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:32:25.132949Z",
     "start_time": "2025-03-06T05:32:25.124566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "file_name = \"test.txt\"\n",
    "\n",
    "with open(file_name, \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ],
   "id": "e1a73f4ad04ec34b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一回  甄士隐梦幻识通灵　贾雨村风尘怀闺秀\n",
      "\n",
      "列位看官：你道此书从何而来？说起根由，虽近荒唐，细按则深有趣味。待在下将此来历注明，方使阅者了然不惑。\n",
      "\n",
      "原来女娲氏炼石补天之时，于大荒山无稽崖炼成高经十二丈、方经二十四丈顽石三万六千五百零一块。娲皇氏只用了\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:32:27.254742Z",
     "start_time": "2025-03-06T05:32:27.248232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#有序、不重复的列表\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#字符和整数之间的投影\n",
    "stoi = {ch : i for i,ch in enumerate(chars)}#符号到整数\n",
    "itos = {i : ch for i,ch in enumerate(chars)}#整数到符号\n",
    "encode = lambda str1:[stoi[c] for c in str1]#编码，把字符串转化为数字串（列表）\n",
    "decode = lambda list1:\"\".join([itos[i] for i in list1])#解码，把数字转换为字符串\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ],
   "id": "803be5b498cfaa3a",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:32:30.338059Z",
     "start_time": "2025-03-06T05:32:30.328921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # token_list = x.tolist()\n",
    "    # for str_list in token_list:\n",
    "    #   print(decode(str_list))\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "get_batch(\"train\")"
   ],
   "id": "7a8033f38cc13737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 65,   2,  87,  96],\n",
       "         [ 27,  63,  55, 101],\n",
       "         [102,  20,  92,  61]]),\n",
       " tensor([[  2,  87,  96,  57],\n",
       "         [ 63,  55, 101,  52],\n",
       "         [ 20,  92,  61,  12]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:30:46.523827Z",
     "start_time": "2025-03-06T05:30:46.514903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "size = 3  #几个值需要做嵌入\n",
    "n_embedding = 4  #嵌入后的维度\n",
    "\n",
    "embedding_table = nn.Embedding(size, n_embedding)\n",
    "# idx = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "idx = torch.arange(3)\n",
    "print(embedding_table(idx))"
   ],
   "id": "829731444b0c85f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674,  0.5349,  0.8094]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:32:42.754771Z",
     "start_time": "2025-03-06T05:32:42.749229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x,y = get_batch(\"train\")\n",
    "print(x)"
   ],
   "id": "fac939f5da2f2451",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 51,  24,  64,  56],\n",
      "        [ 63,  55, 101,  52],\n",
      "        [ 14,   7,   3,  52]])\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:57:11.725936Z",
     "start_time": "2025-03-06T05:57:11.718149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_embd = 3\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "token_embd = token_embedding_table(x)\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "position_idx = torch.arange(block_size)\n",
    "position_emb = position_embedding_table(position_idx)\n",
    "\n",
    "print(\"token_embd\",token_embd)\n",
    "x_list = x.tolist()\n",
    "for str_list in x_list:\n",
    "    decoded_str = decode(str_list)\n",
    "    print(decoded_str)\n",
    "\n",
    "print(\"token_embd\",token_embd)\n",
    "print(\"position_emb\",position_emb)"
   ],
   "id": "f77480a0f59ef76e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embd tensor([[[-1.5322, -0.1141, -0.9761],\n",
      "         [ 0.0521,  0.2882,  0.3520],\n",
      "         [ 0.3848,  0.2943,  0.6351],\n",
      "         [ 0.6860,  0.8626, -0.4877]],\n",
      "\n",
      "        [[-0.2195,  1.5598, -0.0529],\n",
      "         [ 0.9251,  1.6376, -0.9297],\n",
      "         [-0.4352, -1.8566, -1.5076],\n",
      "         [ 0.4865, -0.6963,  1.9266]],\n",
      "\n",
      "        [[ 0.6834,  2.3535,  0.0434],\n",
      "         [ 1.8833, -1.2941, -1.2332],\n",
      "         [ 0.6737, -0.6979,  1.7839],\n",
      "         [ 0.4865, -0.6963,  1.9266]]], grad_fn=<EmbeddingBackward0>)\n",
      "按则深有\n",
      "注明，方\n",
      "二丈、方\n",
      "token_embd tensor([[[-1.5322, -0.1141, -0.9761],\n",
      "         [ 0.0521,  0.2882,  0.3520],\n",
      "         [ 0.3848,  0.2943,  0.6351],\n",
      "         [ 0.6860,  0.8626, -0.4877]],\n",
      "\n",
      "        [[-0.2195,  1.5598, -0.0529],\n",
      "         [ 0.9251,  1.6376, -0.9297],\n",
      "         [-0.4352, -1.8566, -1.5076],\n",
      "         [ 0.4865, -0.6963,  1.9266]],\n",
      "\n",
      "        [[ 0.6834,  2.3535,  0.0434],\n",
      "         [ 1.8833, -1.2941, -1.2332],\n",
      "         [ 0.6737, -0.6979,  1.7839],\n",
      "         [ 0.4865, -0.6963,  1.9266]]], grad_fn=<EmbeddingBackward0>)\n",
      "position_emb tensor([[-0.2809, -1.9549,  0.4061],\n",
      "        [ 1.0392, -0.7863, -0.9167],\n",
      "        [-1.2996, -1.2034,  0.1841],\n",
      "        [ 0.2929,  1.1905, -0.6449]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T05:50:22.062726Z",
     "start_time": "2025-03-06T05:50:22.057638Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "92630c87c363b6a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "execution_count": 64
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
